services:
  web:
    build: .
    image: github-pr-analyzer-web:latest
    container_name: github-pr-analyzer-web
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
    volumes:
      - .:/app
    ports:
      - "8000:8000"
    depends_on:
      - ollama
    env_file:
      - .env

  worker:
    build: .
    image: github-pr-analyzer-worker:latest
    container_name: github-pr-analyzer-worker
    command: celery -A app.workers.tasks worker --loglevel=info -Q default
    volumes:
      - .:/app
    depends_on:
      - ollama
    env_file:
      - .env

  # Ollama model server
  ollama:
    image: ollama/ollama:latest
    container_name: github-pr-analyzer-ollama
    ports:
      - "11434:11434"
    environment:
      OLLAMA_HOST: "0.0.0.0:11434"
      OLLAMA_MODELS: "/models"
    volumes:
      - ollama_models:/models
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/"]
      interval: 10s
      timeout: 5s
      retries: 12

  # One-shot init container: waits for ollama server and pulls Gemma 3 model
  ollama-init:
    image: ollama/ollama:latest
    container_name: github-pr-analyzer-ollama-init
    depends_on:
      - ollama
    environment:
      OLLAMA_MODELS: "/models"
    volumes:
      - ollama_models:/models
    entrypoint: ["/bin/sh", "-c"]
    command: >
      "echo 'waiting for ollama to be ready...' &&
       for i in $(seq 1 120); do
         if curl -sS http://ollama:11434/api >/dev/null 2>&1; then
           echo 'ollama reachable'; break;
         fi;
         echo 'waiting...' && sleep 1;
       done;
       echo 'pulling gemma3:4b (this may take a long time and require lots of disk)'; 
       ollama pull gemma3:4b || ollama pull gemma3;
       echo 'model pull finished'"

volumes:
  ollama_models:
